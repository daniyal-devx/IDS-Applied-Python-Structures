"""
RAG PDF Intelligence - Military-Grade Document Analysis System
Fixed Version with Proper UI
"""

import gradio as gr
import os
from typing import List, Tuple
import traceback

# Import core modules
from core.pdf_loader import PDFLoader
from core.chunking import DocumentChunker
from core.embeddings import EmbeddingGenerator
from core.vector_store import VectorStore
from core.rag_pipeline import RAGPipeline

# Import utilities
from utils.memory import ConversationMemory
from utils.logger import setup_logger, QueryLogger

# Setup logging
logger = setup_logger("RAG_Intelligence")

# Global state
pdf_loader = PDFLoader()
chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)
embedding_generator = None
vector_store = None
rag_pipeline = None
conversation_memory = ConversationMemory()
query_logger = QueryLogger()

# Application state
documents_loaded = False
chunks_data = []


def initialize_models():
    """Initialize AI models (lazy loading)"""
    global embedding_generator, rag_pipeline
    
    try:
        if embedding_generator is None:
            logger.info("Initializing embedding model...")
            embedding_generator = EmbeddingGenerator()
        
        if rag_pipeline is None:
            groq_api_key = os.environ.get("GROQ_API_KEY")
            if not groq_api_key:
                raise ValueError("GROQ_API_KEY environment variable not set")
            logger.info("Initializing RAG pipeline...")
            rag_pipeline = RAGPipeline(groq_api_key)
        
        return True
    except Exception as e:
        logger.error(f"Error initializing models: {str(e)}")
        return False


def process_documents(files):
    """Process uploaded PDF documents"""
    global documents_loaded, chunks_data, vector_store
    
    if not files or len(files) == 0:
        return "âš ï¸ ALERT: No documents uploaded. Please select PDF files.", ""
    
    try:
        # Initialize models
        if not initialize_models():
            return "âŒ SYSTEM ERROR: Failed to initialize AI models. Check API configuration.", ""
        
        # Load PDFs
        logger.info(f"Processing {len(files)} documents...")
        documents = pdf_loader.load_pdfs(files)
        
        if not documents:
            return "âŒ EXTRACTION FAILED: No content extracted from PDFs.", ""
        
        # Chunk documents
        logger.info("Chunking documents...")
        chunks_data = chunker.chunk_documents(documents)
        
        if not chunks_data:
            return "âŒ PROCESSING ERROR: Failed to create document chunks.", ""
        
        # Generate embeddings
        logger.info("Generating embeddings...")
        chunk_texts = [chunk['text'] for chunk in chunks_data]
        embeddings = embedding_generator.generate_embeddings(chunk_texts)
        
        # Build vector store
        logger.info("Building vector index...")
        vector_store = VectorStore(embedding_generator.embedding_dim)
        vector_store.build_index(embeddings, chunks_data)
        
        documents_loaded = True
        
        # Get statistics
        stats = pdf_loader.get_summary_stats()
        chunk_stats = chunker.get_chunking_stats(chunks_data)
        
        status_msg = f"""âœ… PROCESSING COMPLETE

ğŸ“Š INTELLIGENCE REPORT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š Documents Analyzed: {stats['num_files']}
ğŸ“„ Total Pages: {stats['total_pages']}
ğŸ”¤ Characters Processed: {stats['total_characters']:,}
ğŸ§© Data Chunks: {chunk_stats['total_chunks']}
ğŸ“Š Avg Chunk Size: {int(chunk_stats['avg_chunk_size'])} chars
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸŸ¢ SYSTEM READY FOR QUERIES"""
        
        return status_msg, ""
        
    except Exception as e:
        logger.error(f"Error processing documents: {str(e)}")
        traceback.print_exc()
        return f"âŒ CRITICAL ERROR: {str(e)}", ""


def generate_summary(mode):
    """Generate document summary"""
    global chunks_data, rag_pipeline
    
    if not documents_loaded or not chunks_data:
        return "âš ï¸ ALERT: No documents loaded. Upload and process documents first."
    
    try:
        logger.info(f"Generating {mode} summary...")
        summary = rag_pipeline.generate_summary(chunks_data, mode=mode.lower())
        
        formatted_summary = f"""# ğŸ“Š INTELLIGENCE BRIEF ({mode.upper()} MODE)

{summary}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
*Generated by RAG PDF Intelligence System*"""
        
        return formatted_summary
        
    except Exception as e:
        logger.error(f"Error generating summary: {str(e)}")
        return f"âŒ SUMMARY GENERATION FAILED: {str(e)}"


def answer_question(message, history, mode):
    """Answer user question using RAG - FIXED for Gradio chat format"""
    global vector_store, rag_pipeline, conversation_memory, query_logger
    
    # Handle empty message
    if not message or message.strip() == "":
        return history
    
    # Check if documents are loaded
    if not documents_loaded:
        error_msg = "âš ï¸ ALERT: No intelligence data loaded. Process documents before querying."
        history.append((message, error_msg))
        return history
    
    try:
        # Generate query embedding
        query_embedding = embedding_generator.generate_query_embedding(message)
        
        # Retrieve relevant chunks
        retrieved_chunks = vector_store.search(query_embedding, k=5)
        
        # Get conversation context
        context = conversation_memory.get_context_for_llm(num_turns=2)
        
        # Generate answer
        answer, sources = rag_pipeline.generate_answer(
            question=message,
            retrieved_chunks=retrieved_chunks,
            mode=mode.lower(),
            conversation_history=context
        )
        
        # Format answer with sources
        formatted_answer = f"""{answer}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š **SOURCE INTELLIGENCE:**
"""
        for i, source in enumerate(sources, 1):
            formatted_answer += f"\n{i}. ğŸ“„ {source['source']}, Page {source['page']} | Confidence: {source['relevance']:.2%}"
        
        # Update memory
        conversation_memory.add_turn(message, answer, sources)
        
        # Log query
        query_logger.log_query(message, len(retrieved_chunks), True)
        
        # Return updated history - FIXED: append as tuple
        history.append((message, formatted_answer))
        return history
        
    except Exception as e:
        logger.error(f"Error answering question: {str(e)}")
        traceback.print_exc()
        error_msg = f"âŒ QUERY PROCESSING ERROR: {str(e)}"
        query_logger.log_query(message, 0, False)
        history.append((message, error_msg))
        return history


def export_chat_history(format_type):
    """Export conversation history"""
    global conversation_memory
    
    if not conversation_memory.history:
        return None
    
    try:
        if format_type == "Text":
            content = conversation_memory.export_to_text()
            filename = "intelligence_report.txt"
        else:  # JSON
            content = conversation_memory.export_to_json()
            filename = "intelligence_report.json"
        
        # Write to file
        import tempfile
        filepath = os.path.join(tempfile.gettempdir(), filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return filepath
        
    except Exception as e:
        logger.error(f"Error exporting chat history: {str(e)}")
        return None


def clear_conversation():
    """Clear conversation history"""
    global conversation_memory
    conversation_memory.clear()
    return []


def get_analytics():
    """Get query analytics"""
    global query_logger
    
    stats = query_logger.get_stats()
    
    if stats['total_queries'] == 0:
        return "ğŸ“Š No queries processed yet."
    
    return f"""ğŸ“Š **SYSTEM ANALYTICS**

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” Total Queries: {stats['total_queries']}
âœ… Successful: {stats['successful_queries']}
âŒ Failed: {stats['failed_queries']}
ğŸ“ˆ Success Rate: {stats['success_rate']}%
ğŸ“ Avg Query Length: {stats['avg_question_length']} chars
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"""


# Military-Grade CSS
military_css = """
/* Military-Grade Intelligence Theme */
:root {
    --primary-bg: #0a0e1a;
    --secondary-bg: #111827;
    --card-bg: #1a1f2e;
    --border-color: #00d9ff;
    --text-primary: #00ffff;
    --text-secondary: #e0e0e0;
    --accent: #00d9ff;
    --success: #00ff88;
    --warning: #ffaa00;
    --error: #ff4444;
}

body, .gradio-container {
    background: var(--primary-bg) !important;
    color: var(--text-secondary) !important;
    font-family: 'Courier New', monospace !important;
}

/* Header Styling */
.header-box {
    background: linear-gradient(135deg, #0a0e1a 0%, #1a1f2e 100%);
    border: 2px solid var(--border-color);
    border-radius: 8px;
    padding: 2rem;
    text-align: center;
    box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);
    margin-bottom: 2rem;
}

.header-title {
    font-size: 2.5rem;
    font-weight: bold;
    color: var(--text-primary);
    text-shadow: 0 0 10px var(--accent);
    letter-spacing: 0.5rem;
    margin: 0;
}

.header-subtitle {
    color: #888;
    font-size: 0.9rem;
    letter-spacing: 0.3rem;
    margin-top: 0.5rem;
}

/* Section Headers */
.section-header {
    color: var(--text-primary) !important;
    font-weight: bold !important;
    font-size: 1.1rem !important;
    border-bottom: 2px solid var(--border-color) !important;
    padding-bottom: 0.5rem !important;
    margin-bottom: 1rem !important;
}

/* Buttons */
.primary-btn button {
    background: linear-gradient(135deg, #00d9ff, #0088cc) !important;
    color: #000 !important;
    border: none !important;
    font-weight: bold !important;
    text-transform: uppercase !important;
    letter-spacing: 0.1rem !important;
    box-shadow: 0 0 15px rgba(0, 217, 255, 0.5) !important;
}

.primary-btn button:hover {
    box-shadow: 0 0 25px rgba(0, 217, 255, 0.8) !important;
    transform: translateY(-2px);
}

.secondary-btn button {
    background: var(--card-bg) !important;
    color: var(--text-primary) !important;
    border: 2px solid var(--border-color) !important;
}

/* File Upload */
.file-upload {
    border: 2px dashed var(--border-color) !important;
    background: var(--card-bg) !important;
    border-radius: 8px !important;
}

/* Chat Interface */
.chatbot {
    background: var(--secondary-bg) !important;
    border: 2px solid var(--border-color) !important;
    border-radius: 8px !important;
}

/* Status Messages */
.status-box {
    background: var(--card-bg) !important;
    border: 1px solid var(--border-color) !important;
    border-radius: 8px !important;
    padding: 1rem !important;
    font-family: 'Courier New', monospace !important;
}

/* Input Fields */
input, textarea {
    background: var(--card-bg) !important;
    border: 2px solid #333 !important;
    color: var(--text-secondary) !important;
    border-radius: 4px !important;
}

input:focus, textarea:focus {
    border-color: var(--border-color) !important;
    box-shadow: 0 0 10px rgba(0, 217, 255, 0.3) !important;
}

/* Radio Buttons */
.radio-group label {
    color: var(--text-secondary) !important;
}

/* Markdown in Chat */
.message-row {
    font-family: 'Courier New', monospace !important;
}

/* Scrollbar */
::-webkit-scrollbar {
    width: 10px;
    background: var(--secondary-bg);
}

::-webkit-scrollbar-thumb {
    background: var(--border-color);
    border-radius: 5px;
}

::-webkit-scrollbar-thumb:hover {
    background: var(--accent);
}
"""


# Build Gradio Interface
with gr.Blocks(title="RAG PDF Intelligence") as app:
    
    gr.HTML("""
        <div class="header-box">
            <h1 class="header-title">âš¡ RAG PDF INTELLIGENCE</h1>
            <p class="header-subtitle">MILITARY-GRADE DOCUMENT ANALYSIS SYSTEM</p>
        </div>
    """)
    
    with gr.Row():
        # Left Column - Controls
        with gr.Column(scale=1):
            gr.HTML('<p class="section-header">ğŸ“ DOCUMENT UPLOAD</p>')
            
            file_upload = gr.File(
                label="UPLOAD PDF FILES",
                file_count="multiple",
                file_types=[".pdf"],
                elem_classes="file-upload"
            )
            
            process_btn = gr.Button(
                "ğŸš€ PROCESS DOCUMENTS",
                variant="primary",
                elem_classes="primary-btn"
            )
            
            status_output = gr.Markdown(
                label="PROCESSING STATUS",
                elem_classes="status-box"
            )
            
            gr.HTML('<p class="section-header">âš™ï¸ SETTINGS</p>')
            
            mode_selector = gr.Radio(
                choices=["Executive", "Technical"],
                value="Executive",
                label="Answer Mode",
                info="Executive: Strategic insights | Technical: Detailed analysis"
            )
            
            gr.HTML('<p class="section-header">ğŸ“Š DOCUMENT SUMMARY</p>')
            
            summary_btn = gr.Button(
                "Generate Summary",
                variant="secondary",
                elem_classes="secondary-btn"
            )
            
            summary_output = gr.Markdown(
                label="Summary",
                elem_classes="status-box"
            )
            
            gr.HTML('<p class="section-header">ğŸ“¥ EXPORT</p>')
            
            export_format = gr.Radio(
                choices=["Text", "JSON"],
                value="Text",
                label="Format"
            )
            
            export_btn = gr.Button(
                "Download Chat History",
                variant="secondary",
                elem_classes="secondary-btn"
            )
            
            export_file = gr.File(label="Download")
            
            analytics_btn = gr.Button(
                "ğŸ“Š View Analytics",
                variant="secondary",
                elem_classes="secondary-btn"
            )
            
            analytics_output = gr.Markdown()
        
        # Right Column - Chat
        with gr.Column(scale=2):
            gr.HTML('<p class="section-header">ğŸ’¬ INTELLIGENT CHAT INTERFACE</p>')
            
            chatbot = gr.Chatbot(
                label="Conversation",
                height=600,
                elem_classes="chatbot"
            )
            
            with gr.Row():
                msg_input = gr.Textbox(
                    label="Ask a question",
                    placeholder="Enter your intelligence query...",
                    lines=2,
                    scale=4
                )
                
            with gr.Row():
                submit_btn = gr.Button(
                    "Send",
                    variant="primary",
                    scale=1,
                    elem_classes="primary-btn"
                )
                clear_btn = gr.Button(
                    "Clear Conversation",
                    variant="secondary",
                    scale=1,
                    elem_classes="secondary-btn"
                )
    
    # Event Handlers
    process_btn.click(
        fn=process_documents,
        inputs=[file_upload],
        outputs=[status_output, summary_output]
    )
    
    summary_btn.click(
        fn=generate_summary,
        inputs=[mode_selector],
        outputs=[summary_output]
    )
    
    # FIXED: Proper chat interface binding
    submit_btn.click(
        fn=answer_question,
        inputs=[msg_input, chatbot, mode_selector],
        outputs=[chatbot]
    ).then(
        lambda: "",  # Clear input after submit
        None,
        msg_input
    )
    
    msg_input.submit(
        fn=answer_question,
        inputs=[msg_input, chatbot, mode_selector],
        outputs=[chatbot]
    ).then(
        lambda: "",
        None,
        msg_input
    )
    
    clear_btn.click(
        fn=clear_conversation,
        inputs=[],
        outputs=[chatbot]
    )
    
    export_btn.click(
        fn=export_chat_history,
        inputs=[export_format],
        outputs=[export_file]
    )
    
    analytics_btn.click(
        fn=get_analytics,
        inputs=[],
        outputs=[analytics_output]
    )


if __name__ == "__main__":
    logger.info("Starting RAG PDF Intelligence System...")
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        css=military_css
    )